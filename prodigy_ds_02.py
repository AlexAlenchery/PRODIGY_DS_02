# -*- coding: utf-8 -*-
"""PRODIGY_DS_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Wi2AuoJS61xd9czYp2qfXaIzVWX_w7D
"""

from google.colab import files
uploaded_file = files.upload()

import csv
import pandas as pd
df1 = pd.read_csv('diabetes_prediction_dataset.csv')
df1

df1.info()

df1.nunique()

df1.isnull().sum()

import numpy as np
df1['gender'].unique()

df1['gender']=df1['gender'].map({"Female":1,"Male":2,"Other":3})

df1['smoking_history'].unique()

df1['smoking_history']=df1['smoking_history'].map({"never":0,"No Info":1,"current":2,"former":3,"ever":4,"not current":5})

df1.info()

df1.isnull().sum()

"""Performing Data analysis now that preprocessing is completed."""

df1

df1.hist(figsize=(50,30))

import matplotlib.pyplot as plt
import seaborn as sns
sns.distplot(df1['age'],bins=50,color='lightblue',label="Age")
plt.legend()
plt.title("Histogram of age.")

sns.distplot(df1["smoking_history"],bins=25,color="brown",label="")
plt.legend()
plt.title("Smoking history")
#"never":0,"No Info":1,"current":2,"former":3,"ever":4,"not current":5

sns.histplot(df1["diabetes"],bins=25,color="lightgreen")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Assuming df contains your data and the target variable is 'diabetes'
# Selecting independent variables (features) for the regression
X = df1[['gender', 'age', 'hypertension', 'heart_disease', 'smoking_history', 'bmi', 'HbA1c_level', 'blood_glucose_level']]
# Selecting the dependent variable
y = df1['diabetes']

# Convert categorical variables (like gender and smoking_history) to dummy variables
X = pd.get_dummies(X, columns=['gender', 'smoking_history'], drop_first=True)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Creating and fitting the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Predicting on the test set
predictions = log_reg.predict(X_test)

# Evaluating the model (you might want to use different metrics based on your needs)
accuracy = log_reg.score(X_test, y_test)
print(f"Accuracy: {accuracy}")

"""With an accuracy of 0.959, or 95.9%, on the test set, the model appears to do a good job at predicting diabetes using the features that are provided."""

from sklearn.metrics import confusion_matrix, classification_report

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Classification Report
class_report = classification_report(y_test, predictions)
print("\nClassification Report:")
print(class_report)

"""In summary:
Model Execution:

The model performs exceptionally well in predicting non-diabetes cases (class 0) with high precision (97%) and recall (99%), which accounts for its high accuracy (96%).
Its ability to forecast diabetes cases (class 1), however, is not as good. It has a modest proportion of false positives in this category and poorer precision (86%) and recall (62%), suggesting that it misses a substantial number of true diabetes patients.
Expansion and Restraints:

The model suggests a solid generalisation on this element of the data, as evidenced by its robust ability to detect cases without diabetes.
However, there is room for improvement in terms of how well it detects diabetes cases. This may be achieved, for example, by applying more advanced algorithms or attributes relevant to diabetes prediction, or by collecting more representative data.

General Comments:

The model works well for identifying cases without diabetes, but more work may be needed to improve its ability to identify cases of diabetes. Specifically, greater recall without sacrificing precision should be the main goal of this modification.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram of numerical variables
numerical_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']
for col in numerical_cols:
    plt.figure(figsize=(8, 5))
    sns.histplot(df1[col], kde=True, bins=30)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Countplot for categorical variables
categorical_cols = ['gender', 'hypertension', 'heart_disease', 'smoking_history', 'diabetes']
for col in categorical_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=col, data=df1)
    plt.title(f'Countplot of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

# Pairplot for numerical variables (scatter plots and histograms)
sns.pairplot(df1[numerical_cols])
plt.show()

# Correlation heatmap for numerical variables
correlation_matrix = df1[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()